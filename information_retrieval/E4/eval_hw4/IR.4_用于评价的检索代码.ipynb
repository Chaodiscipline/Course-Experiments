{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avdl =  12.324374754484746\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.stem\n",
    "import math\n",
    "from math import log\n",
    "import json\n",
    "\n",
    "\n",
    "tweets_text = []    #tweets_text中的元素为原每个tweet的正文\n",
    "label = []    #label中的元素为原每个tweet的id\n",
    "with open('tweets.txt', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_text.append(tweet['text'])\n",
    "        label.append(tweet['tweetId'])\n",
    "\n",
    "# 分词等预处理\n",
    "def filter(text):\n",
    "    # tokenization\n",
    "    #text1 = text.replace('-', ' ')\n",
    "    filtered1 = word_tokenize(text)\n",
    "    #print(filtered1)\n",
    "\n",
    "    # 去标点\n",
    "    punctuation = [',', '<', '>', '.', \"'s\", '`', '~', '!', '@', '#', '$', '%', '^',\n",
    "                   '&', '*', '(', ')', '-', '_', '=', '+', '[', ']', '{', '}', '\\\\', '|',\n",
    "                   ':', ';', \"\\'\", '/', '?',\"\\\"\",\"“\",\"''\",\"``\"]\n",
    "    filtered2 = [i for i in filtered1 if i not in punctuation]\n",
    "    #print(filtered2)\n",
    "\n",
    "    # Normalization\n",
    "\n",
    "    # Stemming\n",
    "    filtered3 = []\n",
    "    s = nltk.stem.SnowballStemmer('english')\n",
    "    for i in filtered2:\n",
    "        # 大写都变为小写\n",
    "        i.lower()\n",
    "        filtered3.append(s.stem(i))\n",
    "    #print(filtered3)\n",
    "\n",
    "    # 去stopwords\n",
    "    filtered4 = [w for w in filtered3 if w not in stopwords.words('english')]\n",
    "    #print(filtered4)\n",
    "    return(filtered4)\n",
    "\n",
    "\n",
    "textword = []\n",
    "for text in tweets_text:\n",
    "    textword.append(filter(text))\n",
    "\n",
    "\n",
    "\n",
    "# 计算文档平均长度\n",
    "M = len(textword)   #总文档数\n",
    "sum_length = 0\n",
    "for text in textword:\n",
    "    sum_length = sum_length + len(text)  # len(textword[i])就是某一文档的长度\n",
    "avdl = sum_length/M\n",
    "print('avdl = ', avdl)\n",
    "\n",
    "#建立inverted index\n",
    "dict1 = {}\n",
    "label_num = 0\n",
    "for word_list in textword:\n",
    "    for word in word_list:\n",
    "        if word not in dict1:\n",
    "            dict1[word] = []\n",
    "            dict1[word].append([label_num, word_list.count(word)])\n",
    "        elif label != dict1[word][-1][0]:\n",
    "            dict1[word].append([label_num, word_list.count(word)])\n",
    "    label_num = label_num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立queryid到query内容的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{171: 'Ron Weasley birthday', 172: 'Merging of US Air and American', 173: 'muscle pain from statins', 174: 'Hubble oldest star', 175: 'commentary on naming storm Nemo', 176: 'book club members', 177: 'Boko Haram kidnapped French tourists', 178: 'Tiger Woods regains title', 179: 'care of Iditarod dogs', 180: 'Sherlock Elementary BBC CBS', 181: 'Costa Concordia shipwreck', 182: 'Chinua Achebe death', 183: 'Evernote hacked', 184: 'Election of Hugo Chavez successor', 185: 'National Zoo Panda insemination', 186: 'Dorner truck compensation', 187: 'Pope washed Muslims feet', 188: 'Bombing police headquarters Kirkuk', 189: 'injuries by pets', 190: 'Organized crime sports doping Australia', 191: 'Irish laundries apology', 192: 'whooping cough epidemic', 193: 'Bulgarian protesters self immolate', 194: 'cherry blossom Washington', 195: 'Argo wins Oscar', 196: 'US fines Google over Street View ', 197: 'Mad Men season 6', 198: 'Hostess bought by Apollo', 199: 'Ed Koch death', 200: 'UK passes marriage bill', 201: 'Higgs Boson discovery', 202: 'Boko Haram Amnesty opposition', 203: 'Eastern Australia Floods', 204: 'Sotomayor prosecutor racial comments', 205: 'Port Said football riot death sentences', 206: 'yarn bombing', 207: 'David Cameron apology Amritsar', 208: 'Olympics drops wrestling', 209: 'Chelyabinsk meteor damage', 210: 'arrest of Craig Wilson for drive by shooting in D.C.', 211: 'Downton Abbey Lady Mary beau', 212: 'Kate Middleton maternity wear', 213: 'US Embassy in Ankara bombed', 214: 'Common Core math', 215: 'snow blower problems', 216: 'Type II diabetes research', 217: 'Pope candidates', 218: 'Sinkhole rescues', 219: 'Russian meteorite conspiracy', 220: 'Shahbag protest', 221: 'HIV baby cured', 222: 'Oz The Great and Powerful opens', 223: 'dog off leashed', 224: 'dark pool trading', 225: 'Barbara Walters chicken pox'}\n"
     ]
    }
   ],
   "source": [
    "num_query = {}\n",
    "with open('querynumtoquery.txt', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        num_query[int(line[:3])] = line[4:-1]\n",
    "print(num_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检索、打分并排序输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = 0.6\n",
    "k = 1\n",
    "#检索、打分\n",
    "for num in range(171,226):\n",
    "    query = num_query[num]\n",
    "    query_word = filter(query)\n",
    "#     print('query预处理: ', query_word)\n",
    "    score_list = []\n",
    "    for word in query_word:\n",
    "        if word in dict1.keys():\n",
    "            for d in dict1[word]:\n",
    "                # PLNVSM\n",
    "#                 sim = query_word.count(word) * log(1 + log(1+d[1], math.e), math.e) / \\\n",
    "#                         (1 - b + b * len(textword[d[0]])/avdl) * log((M+1)/len(dict1[word]), 2)\n",
    "                # BM25\n",
    "                sim = query_word.count(word) * (k+1)*d[1]/(d[1]+k*(1-b+b*len(textword[d[0]])/avdl)) \\\n",
    "                        * log((M+1)/len(dict1[word]), 2)\n",
    "                judge = True\n",
    "                for i in score_list:\n",
    "                    if i[0] == d[0]:\n",
    "                        judge = False\n",
    "                        i[1] = i[1] + sim\n",
    "                if judge:\n",
    "                    score_list.append([d[0], sim])     # 【文档编号，文档评分】\n",
    "    # 排序输出\n",
    "    score_list.sort(key=lambda x:x[1], reverse = True)\n",
    "    # 最多只输出评分前100的tweetId\n",
    "    score_list100 = score_list[0:min(100,len(score_list))]\n",
    "    s = []   \n",
    "    c = []\n",
    "    for i in score_list:\n",
    "        s.append(i[1])\n",
    "        c.append(label[i[0]])\n",
    "#     print('文档评分: ', s)\n",
    "#     print('对应tweetId: ', c)\n",
    "    # write\n",
    "    with open('result.txt','a+') as f:\n",
    "        for i in range(len(c)):\n",
    "            f.write(str(num)+' '+c[i]+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
